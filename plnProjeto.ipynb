{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Caique de\n",
      "[nltk_data]     Camargo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Caique de\n",
      "[nltk_data]     Camargo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Caique de\n",
      "[nltk_data]     Camargo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package names to C:\\Users\\Caique de\n",
      "[nltk_data]     Camargo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\names.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('names')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[[\"does_this_tweet_contain_hate_speech\", \"tweet_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={ 'does_this_tweet_contain_hate_speech':'Label',\n",
    "                    'tweet_text':'Tweet'}, \n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = df['Label'].astype('category')\n",
    "df['Label'] = df['Label'].cat.rename_categories(['hate','inoffensive','offensive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex(x):\n",
    "    x=' '.join(re.sub(\"@([A-Za-z0-9_À-ÖØ-öø-ÿ.]+)|:|RT|#([A-Za-z0-9_À-ÖØ-öø-ÿ.]+)|\\\"|http([A-Za-z0-9_À-ÖØ-öø-ÿ./:]+)|[À-ÖØ-öø-ÿ�㢉۝ʉӉ~0-9ʁ().!?,]|.co([A-Za-z0-9_À-ÖØ-öø-ÿ./]+)|[_&;//]+\",\" \",x).split())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet'] = df['Tweet'].apply(regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>offensive</td>\n",
       "      <td>Warning penny boards will make you a faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hate</td>\n",
       "      <td>Fuck dykes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hate</td>\n",
       "      <td>at least i dont look like jefree starr faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hate</td>\n",
       "      <td>Is a fag jackie jealous Neeeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>offensive</td>\n",
       "      <td>You heard me bitch but any way I'm back th tex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hate</td>\n",
       "      <td>your a dirty terrorist and your religion is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hate</td>\n",
       "      <td>looking like faggots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hate</td>\n",
       "      <td>Well I thought you knew actually Man why y'all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>offensive</td>\n",
       "      <td>I know It was a joke faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>offensive</td>\n",
       "      <td>I'm tired of people saying I look like my brot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label                                              Tweet\n",
       "0  offensive        Warning penny boards will make you a faggot\n",
       "1       hate                                         Fuck dykes\n",
       "2       hate      at least i dont look like jefree starr faggot\n",
       "3       hate                     Is a fag jackie jealous Neeeee\n",
       "4  offensive  You heard me bitch but any way I'm back th tex...\n",
       "5       hate  your a dirty terrorist and your religion is a ...\n",
       "6       hate                               looking like faggots\n",
       "7       hate  Well I thought you knew actually Man why y'all...\n",
       "8  offensive                        I know It was a joke faggot\n",
       "9  offensive  I'm tired of people saying I look like my brot..."
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate = df[df.Label == 'hate'].values\n",
    "offensive = df[df.Label == 'offensive'].values\n",
    "inoffensive = df[df.Label == 'inoffensive'].values\n",
    "train = np.concatenate((hate[:100],offensive[:100],inoffensive[:100]), axis=0)\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-403-f45bb8bed32e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#all_words = set(word.lower() for passage in train for word in word_tokenize(passage[1]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_words\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-403-f45bb8bed32e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#all_words = set(word.lower() for passage in train for word in word_tokenize(passage[1]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_words\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-403-f45bb8bed32e>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#all_words = set(word.lower() for passage in train for word in word_tokenize(passage[1]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_words\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     return [\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m     ]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     return [\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m     ]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_parentheses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_str\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTARTING_QUOTES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for passage in train:\n",
    "    for word in word_tokenize(passage[1]):\n",
    "        if word not in stopWords:\n",
    "            all_words.append(word.lower())\n",
    "#all_words = set(word.lower() for passage in train for word in word_tokenize(passage[1]))\n",
    "\n",
    "t = [({word: (word in word_tokenize(x[1])) for word in all_words}, x[0]) for x in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 bastard = True           offens : hate   =     25.0 : 1.0\n",
      "                   alike = True           inoffe : offens =     13.4 : 1.0\n",
      "                  faggot = True             hate : offens =      8.7 : 1.0\n",
      "                    look = True           inoffe : hate   =      8.3 : 1.0\n",
      "                    like = True             hate : inoffe =      5.8 : 1.0\n",
      "                 bitches = True           offens : hate   =      5.7 : 1.0\n",
      "                     not = True           offens : hate   =      5.7 : 1.0\n",
      "                   bitch = True           offens : hate   =      5.0 : 1.0\n",
      "                   great = True           inoffe : offens =      5.0 : 1.0\n",
      "                     now = True           offens : hate   =      4.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(t)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inoffensive\n",
      "0.3702385190459238\n",
      "inoffensive\n",
      "0.35599857600569595\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-394-f1206a4ba934>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msent_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_words\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#acc = ({t: (t in all_words)}, classifier.classify(sent_features))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-394-f1206a4ba934>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msent_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_words\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#acc = ({t: (t in all_words)}, classifier.classify(sent_features))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\classify\\naivebayes.py\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\classify\\naivebayes.py\u001b[0m in \u001b[0;36mprob_classify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feature_probdist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[0mfeature_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feature_probdist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m                     \u001b[0mlogprob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfeature_probs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogprob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                     \u001b[1;31m# nb: This case will never come up if the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36mlogprob\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \"\"\"\n\u001b[0;32m    489\u001b[0m         \u001b[1;31m# Default definition, in terms of prob()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0m_NINF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36mprob\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m         \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_freqdist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 842\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_divisor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    843\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test = ['black', 'plus', 'female']\n",
    "for t in test:\n",
    "    sent_features = {word: (word == t) for word in all_words}\n",
    "    #acc = ({t: (t in all_words)}, classifier.classify(sent_features))\n",
    "    classify = classifier.classify(sent_features)\n",
    "    acc = [({word: (word == t)}, classify) for word in all_words]\n",
    "    print(classifier.classify(sent_features))\n",
    "    print(nltk.classify.accuracy(classifier, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.762\n"
     ]
    }
   ],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}\n",
    "\n",
    "from nltk.corpus import names\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "[(name, 'female') for name in names.words('female.txt')])\n",
    "import random\n",
    "random.shuffle(labeled_names)\n",
    "\n",
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "classifier.classify(gender_features('Neo'))\n",
    "classifier.classify(gender_features('Trinity'))\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 'g'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'j'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'p'}, 'male'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'r'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'd'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'r'}, 'male'),\n",
       " ({'last_letter': 's'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'r'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'i'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'd'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 't'}, 'male'),\n",
       " ({'last_letter': 'l'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'o'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'w'}, 'female'),\n",
       " ({'last_letter': 't'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'l'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'h'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'k'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'o'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 't'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'h'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 't'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 's'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'f'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'l'}, 'female'),\n",
       " ({'last_letter': 'r'}, 'female'),\n",
       " ({'last_letter': 't'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 's'}, 'male'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'l'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 's'}, 'male'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 's'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'l'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'o'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'l'}, 'male'),\n",
       " ({'last_letter': 'l'}, 'female'),\n",
       " ({'last_letter': 'd'}, 'male'),\n",
       " ({'last_letter': 'i'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'l'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 'd'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 'l'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'm'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'h'}, 'female'),\n",
       " ({'last_letter': 't'}, 'male'),\n",
       " ({'last_letter': 'f'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 's'}, 'male'),\n",
       " ({'last_letter': 's'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 't'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 's'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 's'}, 'male'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 's'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 'i'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 's'}, 'female'),\n",
       " ({'last_letter': 't'}, 'male'),\n",
       " ({'last_letter': 'd'}, 'male'),\n",
       " ({'last_letter': 's'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'r'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'h'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 't'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 't'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'l'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'o'}, 'male'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'h'}, 'male'),\n",
       " ({'last_letter': 'x'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 't'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'd'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'd'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'l'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 't'}, 'male'),\n",
       " ({'last_letter': 't'}, 'male'),\n",
       " ({'last_letter': 'l'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'l'}, 'male'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 't'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'l'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 's'}, 'male'),\n",
       " ({'last_letter': 't'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'g'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 't'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 's'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'd'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'd'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'm'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'k'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'o'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'l'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 's'}, 'male'),\n",
       " ({'last_letter': 'd'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 's'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'h'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 't'}, 'male'),\n",
       " ({'last_letter': 'f'}, 'male'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'd'}, 'female'),\n",
       " ({'last_letter': 'h'}, 'female'),\n",
       " ({'last_letter': 'h'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'k'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'm'}, 'male'),\n",
       " ({'last_letter': 'd'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 'l'}, 'male'),\n",
       " ({'last_letter': 't'}, 'male'),\n",
       " ({'last_letter': 't'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'g'}, 'female'),\n",
       " ({'last_letter': 'x'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 'r'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'm'}, 'female'),\n",
       " ({'last_letter': 'o'}, 'male'),\n",
       " ({'last_letter': 's'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'd'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 't'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'g'}, 'female'),\n",
       " ({'last_letter': 't'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'l'}, 'female'),\n",
       " ({'last_letter': 'd'}, 'male'),\n",
       " ({'last_letter': 'd'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'l'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'h'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'r'}, 'male'),\n",
       " ({'last_letter': 'd'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 's'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'd'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'h'}, 'male'),\n",
       " ({'last_letter': 'l'}, 'female'),\n",
       " ({'last_letter': 'd'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 't'}, 'male'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'male'),\n",
       " ({'last_letter': 'r'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'o'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'h'}, 'male'),\n",
       " ({'last_letter': 'o'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'v'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'r'}, 'male'),\n",
       " ({'last_letter': 'i'}, 'male'),\n",
       " ({'last_letter': 's'}, 'male'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 's'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'x'}, 'male'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'female'),\n",
       " ({'last_letter': 'l'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 't'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'h'}, 'female'),\n",
       " ({'last_letter': 'i'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'o'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 's'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female')]"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
